# Web Scraper con crawl4ai

[![Python](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-1.2-orange.svg)](https://github.com/whiletruetry/crawler-scraper)

Un web scraper potente y flexible construido con crawl4ai que permite extraer contenido de p√°ginas web de forma s√≠ncrona y as√≠ncrona, con capacidades de scraping simple y recursivo.
Todo esto es posible gracias a los desarrolladores que crearon y mantienen crawl4ai. 
Si necesit√°s algo m√°s complejo, probablemente su framework sea √∫til
No dejes de visitarlos :)
https://docs.crawl4ai.com/


## üöÄ Caracter√≠sticas

- **Extracci√≥n de p√°gina individual (landpage)**: Scraping de una sola p√°gina web
- **Extracci√≥n recursiva (general)**: Scraping de m√∫ltiples p√°ginas siguiendo enlaces internos
- **M√∫ltiples formatos de salida**: markdown, markdown estilizado, HTML limpio, HTML completo
- **Guardado autom√°tico**: Opci√≥n para guardar el contenido extra√≠do en archivos de texto
- **Interfaz de l√≠nea de comandos**: CLI completa con m√∫ltiples opciones
- **Control de profundidad**: Configurable hasta qu√© nivel de enlaces seguir
- **Filtrado inteligente**: Solo extrae enlaces del mismo dominio y evita archivos multimedia
- **Markdown estilizado**: Elimina enlaces y URLs del texto markdown
- **Manejo robusto de errores**: Contin√∫a la extracci√≥n aunque algunas p√°ginas fallen

## üì¶ Instalaci√≥n

### Requisitos y dependencias

- Python 3.7+

```bash
pip install crawl4ai beautifulsoup4
crawl4ai-setup
```

### Instalaci√≥n desde el repositorio

```bash
git clone https://github.com/whiletruetry/crawler-wrapper.git
cd crawler-wrapper
pip install -r requirements.txt
```

## üîß Uso

### L√≠nea de comandos

#### Extraer una sola p√°gina

```bash
# Extracci√≥n b√°sica
python scraper.py landpage https://example.com

# Guardar en archivo
python scraper.py landpage https://example.com --save-path mi_sitio

# Usar formato HTML limpio
python scraper.py landpage https://example.com --result-type cleaned_html

# Solo guardar, no mostrar contenido
python scraper.py landpage https://example.com --save-path docs --no-return
```

#### Extraer m√∫ltiples p√°ginas (recursivo)

```bash
# Extracci√≥n recursiva b√°sica
python scraper.py general https://example.com

# Limitar p√°ginas y profundidad
python scraper.py general https://example.com --max-pages 50 --max-depth 2

# Guardar en markdown limpio
python scraper.py general https://example.com --save-path docs --result-type cleaned_markdown

# Solo guardar archivos
python scraper.py general https://example.com --save-path output --no-return
```

### Uso program√°tico

#### Extracci√≥n as√≠ncrona

```python
import asyncio
from scraper import scrap_landpage, scrap_general

async def main():
    # Extraer una p√°gina
    content = await scrap_landpage("https://example.com")
    print(content)
    
    # Extraer m√∫ltiples p√°ginas
    content_list = await scrap_general(
        "https://example.com", 
        max_pages=20, 
        max_depth=2
    )
    for content in content_list:
        print(f"P√°gina extra√≠da: {len(content)} caracteres")

asyncio.run(main())
```

#### Extracci√≥n s√≠ncrona

```python
from scraper import scrap_landpage_sync, scrap_general_sync

# Extraer una p√°gina
content = scrap_landpage_sync("https://example.com")

# Extraer m√∫ltiples p√°ginas y guardar
content_list = scrap_general_sync(
    "https://example.com",
    max_pages=10,
    max_depth=1,
    save_path="mi_scraping",
    result_type="cleaned_markdown"
)
```

## üìã Opciones de configuraci√≥n

### Tipos de resultado (`result_type`)

| Tipo | Descripci√≥n |
|------|-------------|
| `markdown` | Contenido en formato markdown (predeterminado) |
| `cleaned_markdown` | Markdown sin enlaces `[texto](url)` |
| `html` | HTML completo de la p√°gina |
| `cleaned_html`, `fit_html` | versiones m√°s livianas de HTML |

### Par√°metros principales

| Par√°metro | Descripci√≥n | Predeterminado |
|-----------|-------------|----------------|
| `url` | URL de la p√°gina o sitio web | Requerido |
| `max_pages` | N√∫mero m√°ximo de p√°ginas a extraer | 100 |
| `max_depth` | Profundidad m√°xima de rastreo | 3 |
| `save_path` | Directorio donde guardar archivos | None |
| `return_content` | Si retornar el contenido extra√≠do | True |
| `result_type` | Formato del contenido extra√≠do | "markdown" |

## üìÅ Estructura de archivos guardados

Cuando se especifica `save_path`, los archivos se guardan en:

```
[save_path]/
‚îú‚îÄ‚îÄ 001_index_markdown.txt
‚îú‚îÄ‚îÄ 002_about_markdown.txt
‚îî‚îÄ‚îÄ 003_contact_markdown.txt
```

Cada archivo incluye:
- URL de origen
- Tipo de resultado utilizado
- Contenido extra√≠do

## üõ†Ô∏è Ejemplos avanzados


### Scraping de documentaci√≥n 

```python
import asyncio
from scraper import scrap_general

async def analizar_sitio():
    contenidos = await scrap_general(
        "https://blog.example.com",
        max_pages=50,
        result_type="markdown"
    )
    
    print(f"P√°ginas extra√≠das: {len(contenidos)}")
    print(f"Total caracteres: {total_caracteres}")
    print(f"Promedio por p√°gina: {promedio_caracteres:.0f}")

asyncio.run(analizar_sitio())
```

### Scraping de documentaci√≥n con wrapper s√≠ncrono

```python
# Extraer documentaci√≥n de un sitio
content_list = scrap_general_sync(
    "https://docs.example.com",
    max_pages=200,
    max_depth=4,
    save_path="documentacion_completa",
    result_type="cleaned_markdown"
)
```


## ‚ö†Ô∏è Consideraciones importantes

### Limitaciones t√©cnicas
- Solo extrae contenido del mismo dominio
- No ejecuta JavaScript complejo
- Puede tener problemas con sitios que requieren autenticaci√≥n
- No maneja CAPTCHAs autom√°ticamente


### Errores comunes

**Error: "No module named 'crawl4ai'"**
```bash
pip install crawl4ai
crawl4ai-setup    ## La instalaci√≥n requiere algunos pasos extra, todo est√° indicado en un flujo guiado
```

Dependiendo de tu sistema operativo y el ambiente de ejecuci√≥n, puede haber algunas complicaciones menores en la instalaci√≥n de crawl4ai. Nada demasiado complicado. 

**Error de timeout o conexi√≥n**
- Verifica tu conexi√≥n a internet
- Algunos sitios pueden bloquear requests automatizados
- Intenta con un `max_pages` menor

**Contenido vac√≠o o incompleto**
- Prueba diferentes valores de `result_type`
- Algunos sitios cargan contenido din√°micamente con JavaScript



## üôè Agradecimientos

- [crawl4ai](https://github.com/unclecode/crawl4ai) - La librer√≠a principal de web crawling
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - Para el parsing de HTML
- Comunidad de Python por las incre√≠bles herramientas disponibles



‚≠ê Si este proyecto te fue √∫til, ¬°considera darle una estrella en GitHub!
